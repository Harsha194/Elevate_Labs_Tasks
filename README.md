*****TASK-1*****
# Data Cleaning & Preprocessing - Titanic Dataset
This project demonstrates basic data cleaning and preprocessing for machine learning using the Titanic dataset.
# Data Cleaning & Preprocessing (Titanic Dataset)

## What is Data Preprocessing?
Data preprocessing means turning messy, raw data into clean, organized data that a machine learning model can use.

## Steps Followed
1. **Imported the data** using Pandas.
2. **Explored the data** to understand its structure and missing values.
3. **Handled missing values** by filling in with median (for Age) and mode (for Embarked), and dropped the Cabin column.
4. **Encoded categorical variables** (Sex, Embarked) using one-hot encoding.
5. **Standardized numerical features** (Age, Fare) so they have mean 0 and std 1.
6. **Visualized and removed outliers** in Fare using boxplots and IQR method.

## Tools Used
- Python
- Pandas
- NumPy
- Matplotlib/Seaborn
- Scikit-learn

- Cleaning and transforming the data helps the model learn better and make more accurate predictions.
- ##Outputs regarding to task
- ![Screenshot 2025-05-26 203829](https://github.com/user-attachments/assets/913dfdf7-ffd6-4f57-8473-17a594b5a933)
- ![Screenshot 2025-05-26 203847](https://github.com/user-attachments/assets/b840dc7d-07e1-4030-b669-caa4df8f38e5)
- ![Screenshot 2025-05-26 203913](https://github.com/user-attachments/assets/98178323-b81f-4082-8846-fd7165b9af28)
- ![Screenshot 2025-05-26 203952](https://github.com/user-attachments/assets/9bf0f0a8-694e-4a35-ad62-39ce4d45bab2)
- ![Screenshot 2025-05-26 204005](https://github.com/user-attachments/assets/404a6840-826b-4961-8b55-5b941bf772f3)

*****TASK-2*****

What is EDA?
EDA is the process of analyzing datasets to summarize their main characteristics, often using visual methods.

Steps Followed
Loaded the dataset using Pandas.

Explored basic statistics (mean, median, std, min, max).

Visualized distributions using histograms and boxplots.

Analyzed relationships between features using pairplots and correlation matrices.

Detected anomalies (outliers, missing values, skewed distributions).

Drew basic inferences about feature importance and data quality.

Tools Used
Python

Pandas (Data manipulation)

Matplotlib/Seaborn (Visualizations)

NumPy (Numerical computations)

Key Outputs
‚úÖ Summary Statistics ‚Äì Mean, median, and spread of numerical features (Age, Fare, etc.).
üìä Histograms & Boxplots ‚Äì Visualized distributions and outliers in numeric columns.
üîó Correlation Matrix ‚Äì Identified relationships between features (e.g., Pclass vs. Survival).
‚ö†Ô∏è Outlier Detection ‚Äì Flagged extreme values (e.g., very high Fare values).
üìù Basic Inferences ‚Äì Noted skewness, missing data, and potential preprocessing steps.

Why EDA Matters
Helps identify data issues (missing values, outliers).

Guides feature engineering (which columns need scaling, encoding, etc.).

Improves model performance by ensuring clean, well-understood data.

- Cleaning and transforming the data helps the model learn better and make more accurate predictions.
- ##Outputs regarding to task
- ![Screenshot 2025-05-27 131458](https://github.com/user-attachments/assets/c76590e7-ebef-452d-9a6d-0957667fd42e)
- ![Screenshot 2025-05-27 131534](https://github.com/user-attachments/assets/bcb530c5-e254-4e12-90aa-538148cd5c36)
- ![Screenshot 2025-05-27 131548](https://github.com/user-attachments/assets/6c48b3a8-d18f-4e0b-81b5-333433ec4714)
- ![111](https://github.com/user-attachments/assets/bf72f826-c464-488f-9107-d21eedd4be17)
- ![Screenshot 2025-05-27 131711](https://github.com/user-attachments/assets/95cdd7eb-b5ea-4644-9a56-815483455614)
- ![Screenshot 2025-05-27 131719](https://github.com/user-attachments/assets/334533ab-0dbf-48fb-990b-79425ced5ecc)



*****TASK-3*****

This project implements Simple and Multiple Linear Regression to predict housing prices using the Housing Price Prediction Dataset. The implementation strictly follows machine learning best practices including data preprocessing, model training, evaluation, and interpretation of results.

üìä Dataset Information
Dataset: Housing.csv
Features:

price: Target variable (in ‚Çπ)

area: Total area in sq. ft

bedrooms, bathrooms: Number of rooms

stories: Number of floors

Binary features: mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea

furnishingstatus: Categorical (unfurnished, semi-furnished, furnished)

Size: 545 rows √ó 13 columns

üß∞ Requirements
Python 3.8+

Libraries:

bash
pandas
numpy
scikit-learn
matplotlib
seaborn
üöÄ Installation & Usage
Clone the repository:

bash
git clone https://github.com/yourusername/housing-price-regression.git
cd housing-price-regression
Install dependencies:

bash
pip install -r requirements.txt
Run the analysis:

bash
python housing_regression.py
üß† Key Implementation Steps
Data Preprocessing

Convert binary features (yes/no ‚Üí 1/0)

Label encode furnishingstatus

Train-Test Split (80:20 ratio)

Simple regression: Uses only area feature

Multiple regression: Uses all features

Model Training

LinearRegression from scikit-learn

Separate models for simple and multiple regression

Evaluation Metrics

MAE, MSE, R¬≤ for both models

Visualization & Interpretation

Regression line plot

Feature coefficient analysis

üìà Results & Interpretation
Model Performance
Model Type	R¬≤ Score	MAE (‚Çπ)	Key Insights
Simple Regression	0.28	1,050,000	Limited predictive power using area alone
Multiple Regression	0.67	770,000	2.4x better explanation of price variance
Key Coefficients (Multiple Regression)
Feature	Coefficient (‚Çπ)	Interpretation
area	+3,200	Per sq. ft increase
bathrooms	+500,000	Per additional bathroom
airconditioning	+480,000	AC premium
prefarea	+370,000	Preferred location premium
stories	+300,000	Per additional floor

- ![Screenshot 2025-05-29 102700](https://github.com/user-attachments/assets/e581d01d-c2b2-47b3-a524-cdf2cb64a0d3)
- ![Screenshot 2025-05-29 102713](https://github.com/user-attachments/assets/be22773e-8a49-48d0-9a94-9cf7b6452f74)
- ![Screenshot 2025-05-29 102732](https://github.com/user-attachments/assets/6dd6d1c8-214e-43ed-af57-1e77e9ce4ea8)



*****TASK-4*****


Breast Cancer Classification with Logistic Regression
This project demonstrates binary classification using logistic regression on the Breast Cancer Wisconsin Dataset.

Logistic Regression for Binary Classification
What is Logistic Regression?
Logistic regression predicts the probability of a binary outcome (0/1) using the sigmoid function. It's ideal for medical diagnosis tasks like cancer detection.

Steps Followed
Loaded the data using Pandas and explored features

Preprocessed the data by:

Converting diagnosis (M/B) to binary (1/0)

Dropping unnecessary columns (ID, Unnamed: 32)

Split the data into 70% training and 30% testing sets

Standardized features using Scikit-learn's StandardScaler

Trained the model with Logistic Regression

Evaluated performance using:

Confusion matrix

Precision, recall, and F1-score

ROC-AUC score

Tuned the threshold (default 0.5 ‚Üí tested 0.4)

Visualized results with:

ROC curve

Precision-Recall curve

Sigmoid function

Tools Used
Python

Pandas

NumPy

Matplotlib

Scikit-learn

Key Insights
Achieved 97% accuracy on test data

ROC-AUC score of 0.995 shows excellent class separation

Lowering the threshold to 0.4 increased recall (better for cancer detection)

Visualizations
ROC Curve	Precision-Recall Curve	Sigmoid Function
ROC Curve	Precision-Recall	Sigmoid
"Proper standardization and threshold tuning significantly improve medical diagnosis models."

#outputs regarding to logistic regression

- ![Screenshot 2025-05-30 185325](https://github.com/user-attachments/assets/04bb3efc-a06b-458c-a65d-7c261cf5380c)
- ![Screenshot 2025-05-30 185346](https://github.com/user-attachments/assets/358a5cd6-3898-40b2-86cf-aedcee839b94)
- ![Screenshot 2025-05-30 185404](https://github.com/user-attachments/assets/3c392062-92cc-49f3-b84c-86a820c27e78)
- ![Screenshot 2025-05-30 185418](https://github.com/user-attachments/assets/7f863d91-5dc4-4a25-9308-18d7a19c5866)

*****TASK-5*****


# Decision Trees and Random Forests - Heart Disease Dataset

## What is This Task About?
This project demonstrates how to build, visualize, and evaluate tree-based machine learning models ‚Äî Decision Trees and Random Forests ‚Äî for classification. The goal is to predict the presence of heart disease using clinical features.

## Steps Followed

1. **Data Loading**  
   Loaded the Heart Disease dataset from Kaggle using Pandas.

2. **Data Splitting**  
   Split the dataset into training and testing sets to evaluate model performance.

3. **Training a Decision Tree Classifier**  
   Trained a decision tree model on the training data.

4. **Visualizing the Decision Tree**  
   Used Graphviz and pydotplus to visualize the tree structure for interpretability.

5. **Controlling Overfitting**  
   Limited the tree depth to reduce overfitting and improve generalization.

6. **Training a Random Forest Classifier**  
   Trained a random forest model and compared its accuracy with the decision tree.

7. **Interpreting Feature Importances**  
   Extracted and visualized feature importance scores from the random forest to understand key predictors.

8. **Model Evaluation Using Cross-Validation**  
   Used k-fold cross-validation to get robust estimates of model accuracy.

## Tools Used
- **Python**  
- **Pandas** for data handling  
- **Scikit-learn** for building and evaluating models  
- **Graphviz & pydotplus** for decision tree visualization  
- **Matplotlib** for plotting feature importances  

## Why is This Important?
Tree-based models are powerful and interpretable tools for classification tasks. Visualizing the decision tree helps understand the model‚Äôs decision process, while random forests improve accuracy by reducing overfitting. Evaluating with cross-validation ensures reliable performance estimates.

- ![Screenshot 2025-06-05 142352](https://github.com/user-attachments/assets/6be3342f-2bbf-4665-b7ce-dcd80801f317)
- ![Screenshot 2025-06-05 142421](https://github.com/user-attachments/assets/592236a1-b9b9-40e2-9234-0cf78f3f98e6)


*****TASK-6*****


# K-Nearest Neighbors (KNN) Classification - Iris Dataset

## What is This Task About?
This project demonstrates the implementation of the K-Nearest Neighbors (KNN) algorithm for classification problems. Using the Iris dataset, we explore how to normalize features, train KNN models with different values of K, evaluate their performance, and visualize decision boundaries.

## Steps Followed

1. **Dataset Loading**  
   Loaded the Iris dataset from Scikit-learn‚Äôs built-in datasets.

2. **Feature Normalization**  
   Standardized the features to have mean 0 and standard deviation 1 using `StandardScaler`.

3. **Data Splitting**  
   Split the dataset into training and testing subsets with a 70-30 ratio.

4. **Training KNN Classifier**  
   Trained KNN classifiers with various values of K (1, 3, 5, 7, 9) to observe performance differences.

5. **Model Evaluation**  
   Evaluated models using accuracy scores and visualized the confusion matrix for K=5.

6. **Decision Boundary Visualization**  
   Visualized decision boundaries on the first two features to understand how KNN classifies regions in feature space.

## Tools Used
- **Python**  
- **Pandas** (for data manipulation, optional)  
- **NumPy** for numerical operations  
- **Scikit-learn** for dataset loading, preprocessing, model training, and evaluation  
- **Matplotlib** for visualization  

## Why is This Important?
KNN is a simple yet powerful classification algorithm that uses proximity to make predictions. Normalizing features ensures fair distance calculations. Experimenting with different K values helps balance bias-variance tradeoff. Visualizing decision boundaries aids in understanding model behavior.


- ![Screenshot 2025-06-05 143637](https://github.com/user-attachments/assets/d2033380-b618-43a0-9c8c-fa2adf2ef5cd)
- ![Screenshot 2025-06-05 143653](https://github.com/user-attachments/assets/c61a3064-b9b9-4cde-8de0-b8f3b8b9b8dc)




*****TASK-7*****

# Support Vector Machines (SVM) - Breast Cancer Dataset

## What is This Task About?
This project demonstrates the use of Support Vector Machines (SVMs) for both linear and non-linear classification. Using the Breast Cancer dataset, we train SVM models with linear and RBF kernels, visualize decision boundaries, tune hyperparameters, and evaluate model performance.

## Steps Followed

1. **Dataset Loading**  
   Loaded the Breast Cancer dataset from Scikit-learn‚Äôs built-in datasets for binary classification.

2. **Feature Selection and Scaling**  
   Selected the first two features for visualization and standardized them to zero mean and unit variance.

3. **Training SVM Models**  
   Trained SVM classifiers with linear and RBF kernels on the training data.

4. **Decision Boundary Visualization**  
   Visualized the decision boundaries of both models on 2D feature space to understand their classification behavior.

5. **Hyperparameter Tuning**  
   Used GridSearchCV to tune the regularization parameter `C` and kernel coefficient `gamma` for the RBF kernel.

6. **Model Evaluation**  
   Evaluated the best model on the test set using accuracy and confusion matrix.

7. **Cross-Validation**  
   Performed 5-fold cross-validation on the full dataset with the best model to estimate generalization performance.

## Tools Used
- **Python**  
- **NumPy** for numerical operations  
- **Scikit-learn** for dataset loading, preprocessing, model training, hyperparameter tuning, and evaluation  
- **Matplotlib** for visualization  

## Why is This Important?
SVMs are powerful classifiers that can handle both linear and complex non-linear decision boundaries through kernel functions. Visualizing decision boundaries helps interpret model behavior, while hyperparameter tuning optimizes performance. Cross-validation ensures robust evaluation.



- ![Screenshot 2025-06-05 144727](https://github.com/user-attachments/assets/48b2d5b0-ae7f-4b11-9c3c-af1381146c85)
- ![Screenshot 2025-06-05 144744](https://github.com/user-attachments/assets/c9f492de-55e8-490c-aaf4-3dc8d83c3811)
- ![Screenshot 2025-06-05 144757](https://github.com/user-attachments/assets/43651316-f5f4-4a75-81f7-a88b6609596a)


*****TASK-8*****


# K-Means Clustering - Customer Segmentation Dataset

## What is This Task About?
This project demonstrates unsupervised learning using K-Means clustering to identify distinct customer segments based on purchasing behavior. Using the Mall Customers dataset, we perform data exploration, determine optimal clusters, visualize customer segments, and evaluate clustering quality.

## Steps Followed

### 1. Dataset Loading
- Loaded the Mall Customers dataset containing customer demographics and spending behavior
- Features include: CustomerID, Gender, Age, Annual Income (k$), Spending Score (1-100)

### 2. Data Exploration and Preprocessing
- Analyzed feature distributions through histograms
- Selected relevant features (Annual Income and Spending Score)
- Standardized features to zero mean and unit variance using StandardScaler

### 3. Determining Optimal Clusters
- Applied the Elbow Method to find the optimal number of clusters
- Calculated Silhouette Scores for cluster quality assessment
- Visualized WCSS (Within-Cluster Sum of Squares) vs. number of clusters

### 4. K-Means Clustering
- Performed clustering with the optimal number of clusters
- Visualized clusters with centroids in 2D feature space
- Analyzed cluster characteristics (average age, income, spending score)

### 5. Cluster Evaluation
- Calculated Silhouette Score to evaluate cluster separation
- Interpreted customer segments based on cluster characteristics

### 6. Advanced Techniques (Optional)
- Principal Component Analysis (PCA) for dimensionality reduction
- Experimentation with different cluster counts

## Tools Used
- **Python** as the programming language
- **Pandas** for data manipulation and analysis
- **Scikit-learn** for K-Means implementation, scaling, and metrics
- **Matplotlib/Seaborn** for data visualization
- **NumPy** for numerical operations

## Why is This Important?
- Customer segmentation helps businesses understand different customer groups
- K-Means is a fundamental unsupervised learning algorithm for pattern discovery
- The Elbow Method provides a systematic way to determine cluster count
- Silhouette Score objectively measures clustering quality
- Visualizations make complex clustering results interpretable for stakeholders

## Key Findings
- Optimal number of clusters identified: 5
- Distinct customer segments revealed (e.g., high-income/low-spending, medium-income/medium-spending)
- Silhouette Score of [your_score] indicates [good/moderate/poor] separation



- ![Screenshot 2025-06-14 123301](https://github.com/user-attachments/assets/a3d2ca0b-4c40-4a04-bdb5-b3b77586dcaf)
- ![Screenshot 2025-06-14 123311](https://github.com/user-attachments/assets/096e5bdd-9373-4718-9ff6-50431a87624d)
- ![Screenshot 2025-06-14 123323](https://github.com/user-attachments/assets/beb0f946-68e8-4436-98e2-ec808c411c06)





















